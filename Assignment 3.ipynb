{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907ee547",
   "metadata": {},
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5065779",
   "metadata": {},
   "source": [
    "To read a Hadoop configuration file and display the core components of Hadoop, you'll need to parse the configuration file and extract the relevant information. Hadoop's core components are defined in the core-site.xml and hdfs-site.xml configuration files. Here's a Python program to achieve this using the xml.etree.ElementTree module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3f14a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/hadoop_configuration.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/3395413809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mconfig_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/path/to/hadoop_configuration.xml\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mcore_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_hadoop_configuration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcore_components\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/3395413809.py\u001b[0m in \u001b[0;36mread_hadoop_configuration\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(source, parser)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     \"\"\"\n\u001b[0;32m   1228\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m     \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, source, parser)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/hadoop_configuration.xml'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def read_hadoop_configuration(file_path):\n",
    "    components = set()\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for prop in root.findall(\".//property\"):\n",
    "            name_elem = prop.find(\"name\")\n",
    "            value_elem = prop.find(\"value\")\n",
    "\n",
    "            if name_elem is not None and value_elem is not None:\n",
    "                name = name_elem.text.strip()\n",
    "                value = value_elem.text.strip()\n",
    "\n",
    "                # Check for core components configuration entries\n",
    "                if name == \"fs.defaultFS\":\n",
    "                    components.add(\"HDFS NameNode\")\n",
    "                elif name.startswith(\"dfs.namenode\"):\n",
    "                    components.add(\"HDFS NameNode\")\n",
    "                elif name.startswith(\"dfs.datanode\"):\n",
    "                    components.add(\"HDFS DataNode\")\n",
    "                elif name.startswith(\"dfs.journalnode\"):\n",
    "                    components.add(\"HDFS JournalNode\")\n",
    "                elif name.startswith(\"dfs.zkfc\"):\n",
    "                    components.add(\"HDFS ZKFailoverController\")\n",
    "                elif name.startswith(\"dfs.ha\"):\n",
    "                    components.add(\"HDFS High-Availability\")\n",
    "                elif name.startswith(\"yarn.resourcemanager\"):\n",
    "                    components.add(\"YARN ResourceManager\")\n",
    "                elif name.startswith(\"yarn.nodemanager\"):\n",
    "                    components.add(\"YARN NodeManager\")\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(\"Error parsing the configuration file:\", e)\n",
    "\n",
    "    return components\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_file_path = \"/path/to/hadoop_configuration.xml\"\n",
    "    core_components = read_hadoop_configuration(config_file_path)\n",
    "\n",
    "    if core_components:\n",
    "        print(\"Core Components of Hadoop:\")\n",
    "        for component in core_components:\n",
    "            print(\"-\", component)\n",
    "    else:\n",
    "        print(\"No core components found in the Hadoop configuration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12f837",
   "metadata": {},
   "source": [
    "# 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812302af",
   "metadata": {},
   "source": [
    "To calculate the total file size in a Hadoop Distributed File System (HDFS) directory, you'll need to interact with HDFS using the Hadoop Distributed File System shell commands. Python provides a way to execute shell commands using the subprocess module. You can use this module to execute the hadoop fs commands and then parse the output to calculate the total file size. Here's a Python function to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8a7966",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/3131295902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mhdfs_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/user/hadoop/example_directory\"\u001b[0m  \u001b[1;31m# Replace this with the HDFS directory path you want to calculate the size for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtotal_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hdfs_directory_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdfs_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtotal_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/3131295902.py\u001b[0m in \u001b[0;36mget_hdfs_directory_size\u001b[1;34m(directory_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Execute 'hadoop fs -du' command to get size info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"hadoop\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-du\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Output of the command will contain the total size at the beginning of the line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stderr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_hdfs_directory_size(directory_path):\n",
    "    try:\n",
    "        # Execute 'hadoop fs -du' command to get size info\n",
    "        cmd = [\"hadoop\", \"fs\", \"-du\", \"-s\", directory_path]\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True)\n",
    "\n",
    "        # Output of the command will contain the total size at the beginning of the line\n",
    "        # Extract the size and convert it to bytes\n",
    "        total_size_in_bytes = int(result.stdout.strip().split()[0])\n",
    "\n",
    "        return total_size_in_bytes\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error executing HDFS command:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_directory = \"/user/hadoop/example_directory\"  # Replace this with the HDFS directory path you want to calculate the size for\n",
    "    total_size = get_hdfs_directory_size(hdfs_directory)\n",
    "\n",
    "    if total_size is not None:\n",
    "        print(f\"Total file size in {hdfs_directory}: {total_size} bytes\")\n",
    "    else:\n",
    "        print(\"Failed to calculate the total file size.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323428a6",
   "metadata": {},
   "source": [
    "# 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1e3b1",
   "metadata": {},
   "source": [
    "To extract and display the top N most frequent words from a large text file using the MapReduce approach, we'll simulate the MapReduce process in Python. The MapReduce paradigm consists of two main steps: the Mapper step, where we extract relevant information, and the Reducer step, where we aggregate the results. In this example, we'll perform the MapReduce process in a single Python program.\n",
    "\n",
    "For this task, we'll use the collections.Counter class to count the occurrences of words. Note that this is a simplified implementation, and in a real distributed MapReduce system, the data would be distributed among different nodes for parallel processing.\n",
    "\n",
    "Here's the Python program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c130a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'large_text_file.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/3334811588.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m  \u001b[1;31m# Replace N with the desired number of top frequent words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mtop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/3334811588.py\u001b[0m in \u001b[0;36mmap_reduce\u001b[1;34m(file_path, n)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmap_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Read the content of the text file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'large_text_file.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def mapper(text):\n",
    "    # Split the text into words using regex\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    # Emit each word with count 1\n",
    "    for word in words:\n",
    "        yield word, 1\n",
    "\n",
    "def reducer(word_counts, n):\n",
    "    # Count the occurrences of each word using the Counter class\n",
    "    word_counter = Counter(word_counts)\n",
    "    # Get the top N most common words\n",
    "    top_n_words = word_counter.most_common(n)\n",
    "    return top_n_words\n",
    "\n",
    "def map_reduce(file_path, n):\n",
    "    # Read the content of the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Map step: tokenize and emit word count pairs\n",
    "    mapped_data = mapper(text)\n",
    "\n",
    "    # Reduce step: aggregate word counts\n",
    "    top_n_words = reducer(mapped_data, n)\n",
    "\n",
    "    return top_n_words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text_file_path = \"large_text_file.txt\"  # Replace with the path to your large text file\n",
    "    N = 10  # Replace N with the desired number of top frequent words\n",
    "\n",
    "    top_words = map_reduce(text_file_path, N)\n",
    "\n",
    "    if top_words:\n",
    "        print(f\"Top {N} most frequent words:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word}: {count}\")\n",
    "    else:\n",
    "        print(\"No data or error occurred during the MapReduce process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccc0f2",
   "metadata": {},
   "source": [
    "# 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51945fd6",
   "metadata": {},
   "source": [
    "To check the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API, we'll make HTTP requests to the respective endpoints exposed by Hadoop's HDFS web UI. We'll use the requests library to perform the HTTP requests in Python.\n",
    "\n",
    "Please note that this script assumes that you have access to the Hadoop cluster and its web UI endpoints are reachable from the machine where this script runs.\n",
    "\n",
    "Here's the Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bf38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error checking NameNode health: HTTPConnectionPool(host='your_namenode_hostname', port=50070): Max retries exceeded with url: /jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002775E07EC70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error checking DataNode health: HTTPConnectionPool(host='your_namenode_hostname', port=50070): Max retries exceeded with url: /jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002775E07EBE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_namenode_health(hdfs_web_ui_url):\n",
    "    namenode_health_url = f\"{hdfs_web_ui_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    try:\n",
    "        response = requests.get(namenode_health_url)\n",
    "        response_json = response.json()\n",
    "\n",
    "        live_nodes = response_json['beans'][0]['LiveNodes']\n",
    "        dead_nodes = response_json['beans'][0]['DeadNodes']\n",
    "        safemode_status = response_json['beans'][0]['Safemode']\n",
    "\n",
    "        print(\"NameNode Health Status:\")\n",
    "        print(f\"Live Nodes: {live_nodes}\")\n",
    "        print(f\"Dead Nodes: {dead_nodes}\")\n",
    "        print(f\"Safemode Status: {safemode_status}\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Error checking NameNode health:\", e)\n",
    "\n",
    "def check_datanode_health(hdfs_web_ui_url):\n",
    "    datanode_health_url = f\"{hdfs_web_ui_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    try:\n",
    "        response = requests.get(datanode_health_url)\n",
    "        response_json = response.json()\n",
    "\n",
    "        datanode_count = response_json['beans'][0]['Total']\n",
    "        datanode_live_count = response_json['beans'][0]['Live']\n",
    "\n",
    "        print(\"\\nDataNode Health Status:\")\n",
    "        print(f\"Total DataNodes: {datanode_count}\")\n",
    "        print(f\"Live DataNodes: {datanode_live_count}\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Error checking DataNode health:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_web_ui_url = \"http://your_namenode_hostname:50070\"  # Replace with your HDFS NameNode web UI URL\n",
    "\n",
    "    check_namenode_health(hdfs_web_ui_url)\n",
    "    check_datanode_health(hdfs_web_ui_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7eb95",
   "metadata": {},
   "source": [
    "# 5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206658b6",
   "metadata": {},
   "source": [
    "To list all the files and directories in a specific HDFS path, you can use the subprocess module to execute the hadoop fs -ls command and then parse the output to extract the file and directory names. Here's a Python program to achieve this:\n",
    "\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d02607e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/2422535547.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mhdfs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/user/hadoop/example_directory\"\u001b[0m  \u001b[1;31m# Replace this with the HDFS path you want to list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mfiles_and_directories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_hdfs_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfiles_and_directories\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RAKHIA~1\\AppData\\Local\\Temp/ipykernel_11120/2422535547.py\u001b[0m in \u001b[0;36mlist_hdfs_path\u001b[1;34m(hdfs_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Execute 'hadoop fs -ls' command to list files and directories in the specified path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"hadoop\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Extract file and directory names from the command output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stderr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(hdfs_path):\n",
    "    try:\n",
    "        # Execute 'hadoop fs -ls' command to list files and directories in the specified path\n",
    "        cmd = [\"hadoop\", \"fs\", \"-ls\", hdfs_path]\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True)\n",
    "\n",
    "        # Extract file and directory names from the command output\n",
    "        output_lines = result.stdout.strip().split(\"\\n\")\n",
    "\n",
    "        # Skip the first line (header) and get the names from the rest of the lines\n",
    "        names = [line.split()[-1] for line in output_lines[1:]]\n",
    "\n",
    "        return names\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error executing HDFS command:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_path = \"/user/hadoop/example_directory\"  # Replace this with the HDFS path you want to list\n",
    "\n",
    "    files_and_directories = list_hdfs_path(hdfs_path)\n",
    "\n",
    "    if files_and_directories is not None:\n",
    "        if not files_and_directories:\n",
    "            print(f\"No files or directories found in {hdfs_path}.\")\n",
    "        else:\n",
    "            print(f\"Files and directories in {hdfs_path}:\")\n",
    "            for item in files_and_directories:\n",
    "                print(item)\n",
    "    else:\n",
    "        print(\"Failed to list files and directories in the HDFS path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575d262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
